<li>Performance on Dataset and Market questions is scored using the <a href="https://github.com/forecastingresearch/forecastbench/wiki/Changelog#scoring-method-difficulty-adjusted-brier-score">difficulty-adjusted Brier score <i class="fa-solid fa-arrow-up-right-from-square"></i></a> to account for differences in question difficulty across question sets. The Overall score is then the equal-weighted average of the Dataset and Market scores.</li>
<li>To ensure leaderboard stability, models are included on the leaderboard <a href="https://github.com/forecastingresearch/forecastbench/wiki/Changelog#100-day-delay-before-a-forecaster-is-included-on-the-leaderboard">50 days after forecast submission <i class="fa-solid fa-arrow-up-right-from-square"></i></a>.</li>
<li>Human comparison groups are highlighted in red.</li>
<li>The zero shot and scratchpad prompts used for the models run by ForecastBench can be found on <a href="https://github.com/forecastingresearch/forecastbench/blob/main/src/helpers/llm_prompts.py">GitHub <i class="fa-solid fa-arrow-up-right-from-square"></i></a>.</li>
<li>The ForecastBench baseline forecasters are described on the <a href="https://github.com/forecastingresearch/forecastbench/wiki/Changelog#baseline-forecasters">Changelog <i class="fa-solid fa-arrow-up-right-from-square"></i></a>.</li>
