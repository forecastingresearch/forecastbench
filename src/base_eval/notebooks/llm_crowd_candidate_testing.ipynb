{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1622e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import google.generativeai as genai\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.join(current_path, \"../..\"))\n",
    "from helpers.constants import (\n",
    "    MODEL_TOKEN_LIMITS,\n",
    "    MODEL_NAME_TO_SOURCE,\n",
    ")\n",
    "from helpers.llm_prompts import SCRATCH_PAD_PROMPT, REFORMAT_SINGLE_PROMPT\n",
    "\n",
    "from helpers import model_eval, data_utils, dates, decorator, keys  # noqa: E402\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6b08820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"gpt_4o\": {\"source\": \"OAI\", \"full_name\": \"gpt-4o\"},\n",
    "    \"gpt_4_turbo_0409\": {\"source\": \"OAI\", \"full_name\": \"gpt-4-turbo-2024-04-09\"},\n",
    "    \"llama_3_70b\": {\n",
    "        \"source\": \"TOGETHER\",\n",
    "        \"full_name\": \"meta-llama/Llama-3-70b-chat-hf\",\n",
    "    },\n",
    "    \"mistral_large\": {\n",
    "        \"source\": \"MISTRAL\",\n",
    "        \"full_name\": \"mistral-large-latest\",\n",
    "    },\n",
    "    \"claude_3_opus\": {\"source\": \"ANTHROPIC\", \"full_name\": \"claude-3-opus-20240229\"},\n",
    "    \"qwen_1p5_110b\": {\n",
    "        \"source\": \"TOGETHER\",\n",
    "        \"full_name\": \"Qwen/Qwen1.5-110B-Chat\",\n",
    "    },\n",
    "    # \"gemini_pro\": {\"source\": \"GOOGLE\", \"full_name\": \"gemini-pro\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a0f3c",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6bb57859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 10:47:25,855 WARNING datasets.builder: Found cached dataset json (/Users/apple/.cache/huggingface/datasets/YuehHanChen___json/YuehHanChen--forecasting-0f0bd56f67dc9c3f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4923ec8946408caa4506a787c7c59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset_name = \"YuehHanChen/forecasting\"\n",
    "\n",
    "# Load the dataset, specifying the splits if necessary\n",
    "dataset = load_dataset(path=\"YuehHanChen/forecasting\")\n",
    "\n",
    "mini_val = list(dataset[\"test\"])[:]\n",
    "len(mini_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c35588",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0b5f1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                json_object = json.loads(line)\n",
    "                data.append(json_object)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b6f61de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(index, prompt, model_name, save_dict):\n",
    "    if save_dict[index] != \"\":\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Starting question: {index}\")\n",
    "    prompt = SCRATCH_PAD_PROMPT.format(\n",
    "        question=mini_val[index][\"question\"],\n",
    "        background=mini_val[index][\"background\"],\n",
    "        resolution_criteria=mini_val[index][\"resolution_criteria\"],\n",
    "        close_date=mini_val[index][\"date_resolve_at\"],\n",
    "    )\n",
    "\n",
    "    response = model_eval.get_response_from_model(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1300,\n",
    "        model_name=models[model_name][\"full_name\"],\n",
    "        temperature=0,\n",
    "        wait_time=30,\n",
    "    )\n",
    "\n",
    "    save_dict[index] = model_eval.reformat_answers(response=response, single=True)\n",
    "\n",
    "    logger.info(f\"finished question: {index}, forecast: {save_dict[index]}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def executor(max_workers, prompt, model_name, save_dict):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "\n",
    "        worker_with_args = partial(\n",
    "            worker, prompt=prompt, model_name=model_name, save_dict=save_dict\n",
    "        )\n",
    "        return list(executor.map(worker_with_args, range(len(questions_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fb80b036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 10:47:27,280 INFO root: Running gpt_4o\n",
      "2024-05-15 10:47:27,281 INFO root: Running gpt_4_turbo_0409\n",
      "2024-05-15 10:47:27,282 INFO root: Running llama_3_70b\n",
      "2024-05-15 10:47:27,282 INFO root: Running mistral_large\n",
      "2024-05-15 10:47:27,282 INFO root: Running claude_3_opus\n",
      "2024-05-15 10:47:27,283 INFO root: Running qwen_1p5_110b\n"
     ]
    }
   ],
   "source": [
    "base = \"llm_crowd_candidate_eval/\"\n",
    "all_prompts = [SCRATCH_PAD_PROMPT]\n",
    "\n",
    "results = {}\n",
    "questions_list = [d[\"question\"] for d in mini_val]\n",
    "model_result_loaded = {}\n",
    "\n",
    "for prompt_index in range(len(all_prompts)):\n",
    "    for model in models:\n",
    "        if model not in model_result_loaded.keys():\n",
    "            model_result_loaded[model] = {}\n",
    "        model_result_loaded[model][f\"prompt_{prompt_index}\"] = False\n",
    "\n",
    "for prompt_index in range(len(all_prompts)):\n",
    "    for model in models:\n",
    "        file_path = f\"{base}/{prompt_index}/{model}.jsonl\"\n",
    "\n",
    "        if model not in results.keys():\n",
    "            results[model] = {}\n",
    "        try:\n",
    "            results[model] = read_jsonl(file_path)\n",
    "            model_result_loaded[model][\n",
    "                f\"prompt_{prompt_index}\"\n",
    "            ] = True  # Set flag to True if loaded successfully\n",
    "        except:\n",
    "            results[model][f\"prompt_{prompt_index}\"] = {i: \"\" for i in range(len(questions_list))}\n",
    "\n",
    "for prompt_index in range(len(all_prompts)):\n",
    "    for model, info in models.items():\n",
    "        # only execute the model if we have not had its results yet\n",
    "        logger.info(f\"Running {model}\")\n",
    "        if not model_result_loaded[model][f\"prompt_{prompt_index}\"]:\n",
    "            executor_count = 30\n",
    "            executor(\n",
    "                executor_count,\n",
    "                all_prompts[prompt_index],\n",
    "                model,\n",
    "                results[model][f\"prompt_{prompt_index}\"],\n",
    "            )\n",
    "\n",
    "for prompt_index in range(len(all_prompts)):\n",
    "    for model in models:\n",
    "        file_path = f\"{base}/{prompt_index}/{model}.jsonl\"\n",
    "        if not model_result_loaded[model][f\"prompt_{prompt_index}\"]:\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            with open(file_path, \"w\") as f:\n",
    "                json.dump(results[model][f\"prompt_{prompt_index}\"], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1243dd3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "aa7fb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in results.keys():\n",
    "    refuse_to_answer_cnt = 0\n",
    "    for key, answer in results[model][0].items():\n",
    "        if answer == None:\n",
    "            answer = 0.5\n",
    "            refuse_to_answer_cnt += 1\n",
    "        results[model][0][key] = answer\n",
    "\n",
    "    results[model][0][\"refuse_to_answer_cnt\"] = refuse_to_answer_cnt\n",
    "\n",
    "for model in results.keys():\n",
    "    results[model] = results[model][0]\n",
    "\n",
    "\n",
    "def brier_score(prediction, answer):\n",
    "    return (prediction - answer) ** 2\n",
    "\n",
    "\n",
    "# aggregation\n",
    "agg_results = {}\n",
    "for agg_type in [\"mean\", \"median\", \"trimmed_mean\", \"geometric_mean\", \"geometric_mean_log_odds\"]:\n",
    "    agg_results[agg_type] = {}\n",
    "    for model in results.keys():\n",
    "        for key, answer in results[model].items():\n",
    "            if key not in agg_results[agg_type]:\n",
    "                agg_results[agg_type][key] = [answer]\n",
    "            else:\n",
    "                agg_results[agg_type][key].append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d44ce7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def trimmed_mean(probabilities):\n",
    "    # Sort the list of probabilities\n",
    "\n",
    "    sorted_probs = sorted(probabilities)\n",
    "\n",
    "    # Remove the smallest and largest probabilities\n",
    "    trimmed_probs = sorted_probs[1:-1]\n",
    "\n",
    "    # Calculate the mean of the remaining probabilities\n",
    "    trimmed_mean_value = sum(trimmed_probs) / len(trimmed_probs)\n",
    "\n",
    "    return trimmed_mean_value\n",
    "\n",
    "\n",
    "def geometric_mean(numbers):\n",
    "    if not numbers:\n",
    "        return 0  # Return 0 for an empty list to avoid math domain error\n",
    "    product = 1.0\n",
    "    for number in numbers:\n",
    "        product *= number\n",
    "    return product ** (1 / len(numbers))\n",
    "\n",
    "\n",
    "def geometric_mean_log_odds(probs):\n",
    "    # Convert probabilities to log odds\n",
    "    log_odds = np.log(np.array(probs) / (1 - np.array(probs)))\n",
    "\n",
    "    # Compute the geometric mean of the log odds\n",
    "    mean_log_odds = np.mean(log_odds)\n",
    "\n",
    "    # Convert the mean log odds back to probability\n",
    "    combined_prob = np.exp(mean_log_odds) / (1 + np.exp(mean_log_odds))\n",
    "\n",
    "    return combined_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3f1453ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation\n",
    "for agg_type in [\"mean\", \"median\", \"trimmed_mean\", \"geometric_mean\", \"geometric_mean_log_odds\"]:\n",
    "    for key, answers in agg_results[agg_type].items():\n",
    "        if key != \"refuse_to_answer_cnt\":\n",
    "            if agg_type == \"mean\":\n",
    "                agg_results[agg_type][key] = np.mean(answers)\n",
    "            elif agg_type == \"median\":\n",
    "                agg_results[agg_type][key] = np.median(answers)\n",
    "            elif agg_type == \"trimmed_mean\":\n",
    "                agg_results[agg_type][key] = trimmed_mean(answers)\n",
    "            elif agg_type == \"geometric_mean\":\n",
    "                agg_results[agg_type][key] = geometric_mean(answers)\n",
    "            elif agg_type == \"geometric_mean_log_odds\":\n",
    "                agg_results[agg_type][key] = geometric_mean_log_odds(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0dfd1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_scores = pd.DataFrame()\n",
    "\n",
    "results.update(agg_results)\n",
    "\n",
    "for model in results.keys():\n",
    "    brier_scores_model = []\n",
    "    for question_id, prediction in results[model].items():\n",
    "        if question_id != \"refuse_to_answer_cnt\":\n",
    "            brier_score_value = brier_score(\n",
    "                float(prediction), mini_val[int(question_id)][\"resolution\"]\n",
    "            )\n",
    "            brier_scores_model.append(brier_score_value)\n",
    "\n",
    "    avg_brier_score = sum(brier_scores_model) / len(brier_scores_model)\n",
    "    brier_scores.at[model, f\"Scratchpad\"] = avg_brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "54f9d2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_4o's refuse-to-answer count: 0\n",
      "gpt_4_turbo_0409's refuse-to-answer count: 0\n",
      "llama_3_70b's refuse-to-answer count: 1\n",
      "mistral_large's refuse-to-answer count: 1\n",
      "claude_3_opus's refuse-to-answer count: 7\n",
      "qwen_1p5_110b's refuse-to-answer count: 0\n",
      "mean's refuse-to-answer count: [0, 0, 1, 1, 7, 0]\n",
      "median's refuse-to-answer count: [0, 0, 1, 1, 7, 0]\n",
      "trimmed_mean's refuse-to-answer count: [0, 0, 1, 1, 7, 0]\n",
      "geometric_mean's refuse-to-answer count: [0, 0, 1, 1, 7, 0]\n",
      "geometric_mean_log_odds's refuse-to-answer count: [0, 0, 1, 1, 7, 0]\n"
     ]
    }
   ],
   "source": [
    "for model in results:\n",
    "    print(f\"{model}'s refuse-to-answer count: {results[model]['refuse_to_answer_cnt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "15af39f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scratchpad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt_4o</th>\n",
       "      <td>0.187390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4_turbo_0409</th>\n",
       "      <td>0.204570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_3_70b</th>\n",
       "      <td>0.219266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_large</th>\n",
       "      <td>0.224858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3_opus</th>\n",
       "      <td>0.215378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_1p5_110b</th>\n",
       "      <td>0.209576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.201707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.201373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trimmed_mean</th>\n",
       "      <td>0.201978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geometric_mean</th>\n",
       "      <td>0.199454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geometric_mean_log_odds</th>\n",
       "      <td>0.200971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Scratchpad\n",
       "gpt_4o                     0.187390\n",
       "gpt_4_turbo_0409           0.204570\n",
       "llama_3_70b                0.219266\n",
       "mistral_large              0.224858\n",
       "claude_3_opus              0.215378\n",
       "qwen_1p5_110b              0.209576\n",
       "mean                       0.201707\n",
       "median                     0.201373\n",
       "trimmed_mean               0.201978\n",
       "geometric_mean             0.199454\n",
       "geometric_mean_log_odds    0.200971"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947eed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
