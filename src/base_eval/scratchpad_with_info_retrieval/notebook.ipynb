{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c7dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/envs/myenv-llm-bench/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Notebook for scratchpad with info retrieval.\"\"\"\n",
    "\n",
    "# flake8: noqa\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.join(current_path, \"../..\"))  # noqa: E402\n",
    "from helpers import data_utils, model_eval  # noqa: E402\n",
    "from helpers.llm_prompts import (  # noqa: E402\n",
    "    HUMAN_JOINT_PROMPT_1,\n",
    "    HUMAN_JOINT_PROMPT_2,\n",
    "    HUMAN_JOINT_PROMPT_3,\n",
    "    HUMAN_JOINT_PROMPT_4,\n",
    "    SCRATCH_PAD_WITH_SUMMARIES_JOINT_QUESTION_PROMPT,\n",
    "    SCRATCH_PAD_WITH_SUMMARIES_MARKET_PROMPT,\n",
    "    SCRATCH_PAD_WITH_SUMMARIES_NON_MARKET_PROMPT,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c71ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_joint_prompts = [\n",
    "    HUMAN_JOINT_PROMPT_1,\n",
    "    HUMAN_JOINT_PROMPT_2,\n",
    "    HUMAN_JOINT_PROMPT_3,\n",
    "    HUMAN_JOINT_PROMPT_4,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d787e6",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0853b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"latest-llm.jsonl\"\n",
    "questions_data = data_utils.read_jsonl(file_path)[0]\n",
    "questions = questions_data[\"questions\"]\n",
    "\n",
    "single_non_acled_questions = [\n",
    "    q for q in questions if q[\"combination_of\"] == \"N/A\" and q[\"source\"] != \"acled\"\n",
    "]\n",
    "single_acled_questions = [\n",
    "    q for q in questions if q[\"combination_of\"] == \"N/A\" and q[\"source\"] == \"acled\"\n",
    "]\n",
    "combo_questions = [q for q in questions if q[\"combination_of\"] != \"N/A\" and q[\"source\"] == \"acled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d850050",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_questions_unrolled = []\n",
    "\n",
    "for q in combo_questions:\n",
    "    for i in range(4):\n",
    "        new_q = q.copy()\n",
    "        new_q[\"combo_index\"] = i\n",
    "\n",
    "        combo_questions_unrolled.append(new_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adcf14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339, 162, 644)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_non_acled_questions), len(single_acled_questions), len(combo_questions_unrolled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45413a4e",
   "metadata": {},
   "source": [
    "### Scratchpad + Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e11c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"gpt_3p5_turbo_0125\": {\"source\": \"OAI\", \"full_name\": \"gpt-3.5-turbo-0125\"},\n",
    "    \"gpt_4_turbo_0409\": {\"source\": \"OAI\", \"full_name\": \"gpt-4-turbo-2024-04-09\"},\n",
    "    \"gpt_4_1106_preview\": {\"source\": \"OAI\", \"full_name\": \"gpt-4-1106-preview\"},\n",
    "    \"gpt_4_0125_preview\": {\"source\": \"OAI\", \"full_name\": \"gpt-4-0125-preview\"},\n",
    "    \"gpt_4o\": {\"source\": \"OAI\", \"full_name\": \"gpt-4o\"},\n",
    "    \"mistral_8x7b_instruct\": {\n",
    "        \"source\": \"TOGETHER\",\n",
    "        \"full_name\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    },\n",
    "    \"mistral_8x22b_instruct\": {\n",
    "        \"source\": \"TOGETHER\",\n",
    "        \"full_name\": \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "    },\n",
    "    \"mistral_large\": {\n",
    "        \"source\": \"MISTRAL\",\n",
    "        \"full_name\": \"mistral-large-latest\",\n",
    "    },\n",
    "    \"qwen_1p5_110b\": {\n",
    "        \"source\": \"TOGETHER\",\n",
    "        \"full_name\": \"Qwen/Qwen1.5-110B-Chat\",\n",
    "    },\n",
    "    \"claude_2p1\": {\"source\": \"ANTHROPIC\", \"full_name\": \"claude-2.1\"},\n",
    "    \"claude_3_opus\": {\"source\": \"ANTHROPIC\", \"full_name\": \"claude-3-opus-20240229\"},\n",
    "    \"claude_3_sonnet\": {\"source\": \"ANTHROPIC\", \"full_name\": \"claude-3-sonnet-20240229\"},\n",
    "    \"claude_3_haiku\": {\"source\": \"ANTHROPIC\", \"full_name\": \"claude-3-haiku-20240307\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6500e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping retrieved summaries back into questions\n",
    "for question_source in [single_non_acled_questions, single_acled_questions]:\n",
    "    for q in question_source:\n",
    "        reformatted_id = q[\"id\"].replace(\"/\", \"_\")\n",
    "        filename = f\"news/{reformatted_id}.pickle\"\n",
    "        with open(filename, \"rb\") as file:\n",
    "            retrieved_info = pickle.load(file)\n",
    "        q[\"news\"] = retrieved_info\n",
    "\n",
    "for q in combo_questions_unrolled:\n",
    "    for sub_q in q[\"combination_of\"]:\n",
    "        reformatted_id = sub_q[\"id\"].replace(\"/\", \"_\")\n",
    "        filename = f\"news/{reformatted_id}.pickle\"\n",
    "        with open(filename, \"rb\") as file:\n",
    "            retrieved_info = pickle.load(file)\n",
    "        sub_q[\"news\"] = retrieved_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca6b8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_retrieved_info(all_retrieved_info):\n",
    "    \"\"\"Get all retrieved news.\"\"\"\n",
    "    retrieved_info = \"\"\n",
    "    for summary in all_retrieved_info:\n",
    "        retrieved_info += f\"Article title: {summary['title']}\" + \"\\n\"\n",
    "        retrieved_info += f\"Summary: {summary['summary']}\" + \"\\n\\n\"\n",
    "    return retrieved_info\n",
    "\n",
    "\n",
    "def worker(index, model_name, save_dict, questions_to_eval):\n",
    "    \"\"\"Worker for one inference.\"\"\"\n",
    "    if save_dict[index] != \"\":\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Starting {model_name} - {index}\")\n",
    "\n",
    "    if questions_to_eval[index][\"source\"] != \"acled\":\n",
    "        prompt = SCRATCH_PAD_WITH_SUMMARIES_MARKET_PROMPT.format(\n",
    "            question=questions_to_eval[index][\"question\"],\n",
    "            background=questions_to_eval[index][\"background\"]\n",
    "            + \"\\n\"\n",
    "            + questions_to_eval[index][\"model_info_resolution_criteria\"],\n",
    "            resolution_criteria=questions_to_eval[index][\"resolution_criteria\"],\n",
    "            close_date=questions_to_eval[index][\"model_info_close_datetime\"],\n",
    "            retrieved_info=get_all_retrieved_info(questions_to_eval[index][\"news\"]),\n",
    "        )\n",
    "        response = model_eval.get_response_from_model(\n",
    "            prompt=prompt,\n",
    "            max_tokens=2000,\n",
    "            model_name=models[model_name][\"full_name\"],\n",
    "            temperature=0,\n",
    "            wait_time=30,\n",
    "        )\n",
    "\n",
    "        save_dict[index] = (model_eval.reformat_answers(response=response, single=True), response)\n",
    "\n",
    "    else:\n",
    "        all_resolution_dates = []\n",
    "        for horizon in questions_to_eval[index][\"forecast_horizons\"]:\n",
    "            resolution_date = datetime.fromisoformat(\n",
    "                questions_to_eval[index][\"freeze_datetime\"]\n",
    "            ) + timedelta(days=7 + horizon)\n",
    "            resolution_date = resolution_date.isoformat()\n",
    "            all_resolution_dates.append(resolution_date)\n",
    "\n",
    "        if questions_to_eval[index][\"combination_of\"] == \"N/A\":\n",
    "            prompt = SCRATCH_PAD_WITH_SUMMARIES_NON_MARKET_PROMPT.format(\n",
    "                question=questions_to_eval[index][\"question\"],\n",
    "                background=questions_to_eval[index][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"model_info_resolution_criteria\"],\n",
    "                resolution_criteria=questions_to_eval[index][\"resolution_criteria\"],\n",
    "                freeze_datetime=questions_to_eval[index][\"freeze_datetime\"],\n",
    "                freeze_datetime_value=questions_to_eval[index][\"freeze_datetime_value\"],\n",
    "                freeze_datetime_value_explanation=questions_to_eval[index][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                retrieved_info=get_all_retrieved_info(questions_to_eval[index][\"news\"]),\n",
    "                list_of_resolution_dates=all_resolution_dates,\n",
    "            )\n",
    "            response = model_eval.get_response_from_model(\n",
    "                prompt=prompt,\n",
    "                max_tokens=2000,\n",
    "                model_name=models[model_name][\"full_name\"],\n",
    "                temperature=0,\n",
    "                wait_time=30,\n",
    "            )\n",
    "            save_dict[index] = (\n",
    "                model_eval.reformat_answers(\n",
    "                    response=response, prompt=prompt, question=questions_to_eval[index]\n",
    "                ),\n",
    "                response,\n",
    "            )\n",
    "        else:\n",
    "            prompt = SCRATCH_PAD_WITH_SUMMARIES_JOINT_QUESTION_PROMPT.format(\n",
    "                human_prompt=human_joint_prompts[questions_to_eval[index][\"combo_index\"]],\n",
    "                question_1=questions_to_eval[index][\"combination_of\"][0][\"question\"],\n",
    "                question_2=questions_to_eval[index][\"combination_of\"][1][\"question\"],\n",
    "                background_1=questions_to_eval[index][\"combination_of\"][0][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][0][\"model_info_resolution_criteria\"],\n",
    "                background_2=questions_to_eval[index][\"combination_of\"][1][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][1][\"model_info_resolution_criteria\"],\n",
    "                resolution_criteria_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                resolution_criteria_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                freeze_datetime_1=questions_to_eval[index][\"combination_of\"][0][\"freeze_datetime\"],\n",
    "                freeze_datetime_2=questions_to_eval[index][\"combination_of\"][1][\"freeze_datetime\"],\n",
    "                freeze_datetime_value_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"freeze_datetime_value\"\n",
    "                ],\n",
    "                freeze_datetime_value_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"freeze_datetime_value\"\n",
    "                ],\n",
    "                freeze_datetime_value_explanation_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                freeze_datetime_value_explanation_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                retrieved_info_1=get_all_retrieved_info(\n",
    "                    questions_to_eval[index][\"combination_of\"][0][\"news\"]\n",
    "                ),\n",
    "                retrieved_info_2=get_all_retrieved_info(\n",
    "                    questions_to_eval[index][\"combination_of\"][1][\"news\"]\n",
    "                ),\n",
    "                list_of_resolution_dates=all_resolution_dates,\n",
    "            )\n",
    "\n",
    "            response = model_eval.get_response_from_model(\n",
    "                prompt=prompt,\n",
    "                max_tokens=2000,\n",
    "                model_name=models[model_name][\"full_name\"],\n",
    "                temperature=0,\n",
    "                wait_time=30,\n",
    "            )\n",
    "\n",
    "            save_dict[index] = (\n",
    "                model_eval.reformat_answers(\n",
    "                    response=response, prompt=prompt, question=questions_to_eval[index]\n",
    "                ),\n",
    "                response,\n",
    "            )\n",
    "\n",
    "    logger.info(f\"Answer: {save_dict[index][0]}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def executor(max_workers, model_name, save_dict, questions_to_eval):\n",
    "    \"\"\"Executor to run all inferences.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        worker_with_args = partial(\n",
    "            worker, model_name=model_name, save_dict=save_dict, questions_to_eval=questions_to_eval\n",
    "        )\n",
    "        return list(executor.map(worker_with_args, range(len(questions_to_eval))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fe9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "model_result_loaded = {}\n",
    "models_to_test = list(models.keys())[:]\n",
    "\n",
    "\n",
    "for question in [single_acled_questions, combo_questions_unrolled, single_non_acled_questions]:\n",
    "    questions_to_eval = question\n",
    "    if question[0][\"source\"] != \"acled\":\n",
    "        test_type = \"scratchpad_with_info_retrieval/non_acled\"\n",
    "    elif question[0][\"source\"] == \"acled\" and question[0][\"combination_of\"] == \"N/A\":\n",
    "        test_type = \"scratchpad_with_info_retrieval/acled\"\n",
    "    else:\n",
    "        test_type = \"scratchpad_with_info_retrieval/combo\"\n",
    "\n",
    "    for model in models_to_test:\n",
    "        if model not in model_result_loaded.keys():\n",
    "            model_result_loaded[model] = {}\n",
    "        model_result_loaded[model] = False\n",
    "\n",
    "    for model in models_to_test:\n",
    "        file_path = f\"{test_type}/{model}.jsonl\"\n",
    "        if model not in results.keys():\n",
    "            results[model] = {}\n",
    "        try:\n",
    "            results[model] = read_jsonl(file_path)\n",
    "            model_result_loaded[model] = True  # Set flag to True if loaded successfully\n",
    "        except:\n",
    "            results[model] = {i: \"\" for i in range(len(questions_to_eval))}\n",
    "\n",
    "    for model in models_to_test:\n",
    "        file_path = f\"{test_type}/{model}.jsonl\"\n",
    "        if not model_result_loaded[model]:\n",
    "            executor_count = 50\n",
    "            if models[model][\"source\"] == \"ANTHROPIC\":\n",
    "                executor_count = 30\n",
    "\n",
    "            executor(executor_count, model, results[model], questions_to_eval)\n",
    "\n",
    "            current_model_forecasts = []\n",
    "            for index in range(len(questions_to_eval)):\n",
    "                if questions_to_eval[index][\"source\"] == \"acled\":\n",
    "                    for forecast, horizon in zip(\n",
    "                        results[model][index][0], questions_to_eval[index][\"forecast_horizons\"]\n",
    "                    ):\n",
    "                        current_forecast = {\n",
    "                            \"id\": questions_to_eval[index][\"id\"],\n",
    "                            \"source\": questions_to_eval[index][\"source\"],\n",
    "                            \"forecast\": forecast,\n",
    "                            \"horizon\": horizon,\n",
    "                            \"reasoning\": results[model][index][1],\n",
    "                        }\n",
    "\n",
    "                        if questions_to_eval[index][\"combination_of\"] != \"N/A\":\n",
    "                            combo_index = questions_to_eval[index][\"combo_index\"]\n",
    "                            if combo_index == 0:\n",
    "                                current_forecast[\"direction\"] = [1, 1]\n",
    "                            elif combo_index == 1:\n",
    "                                current_forecast[\"direction\"] = [1, -1]\n",
    "                            elif combo_index == 2:\n",
    "                                current_forecast[\"direction\"] = [-1, 1]\n",
    "                            else:\n",
    "                                current_forecast[\"direction\"] = [-1, -1]\n",
    "\n",
    "                        current_model_forecasts.append(current_forecast)\n",
    "\n",
    "                else:\n",
    "                    current_forecast = {\n",
    "                        \"id\": questions_to_eval[index][\"id\"],\n",
    "                        \"source\": questions_to_eval[index][\"source\"],\n",
    "                        \"forecast\": results[model][index][0],\n",
    "                        \"reasoning\": results[model][index][1],\n",
    "                    }\n",
    "                    current_model_forecasts.append(current_forecast)\n",
    "\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            with open(file_path, \"w\") as file:\n",
    "                for entry in current_model_forecasts:\n",
    "                    json_line = json.dumps(entry)\n",
    "                    file.write(json_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08cb2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.generage_final_forecast_files(\n",
    "    deadline=questions_data[\"forecast_due_date\"],\n",
    "    prompt_type=\"scratchpad_with_info_retrieval\",\n",
    "    models=models,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (myenv-llm-bench)",
   "language": "python",
   "name": "myenv-llm-bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
