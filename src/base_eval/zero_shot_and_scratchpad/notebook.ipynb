{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c7dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Notebook for zero shot and scratchpad eval.\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.join(current_path, \"../..\"))  # noqa: E402\n",
    "from helpers import (  # noqa: E402\n",
    "    constants,\n",
    "    data_utils,\n",
    "    env,\n",
    "    model_eval,\n",
    "    question_curation,\n",
    ")\n",
    "from helpers.llm_prompts import (  # noqa: E402\n",
    "    HUMAN_JOINT_PROMPT_1,\n",
    "    HUMAN_JOINT_PROMPT_2,\n",
    "    HUMAN_JOINT_PROMPT_3,\n",
    "    HUMAN_JOINT_PROMPT_4,\n",
    "    SCRATCH_PAD_MARKET_JOINT_QUESTION_PROMPT,\n",
    "    SCRATCH_PAD_MARKET_PROMPT,\n",
    "    SCRATCH_PAD_NON_MARKET_JOINT_QUESTION_PROMPT,\n",
    "    SCRATCH_PAD_NON_MARKET_PROMPT,\n",
    "    ZERO_SHOT_MARKET_JOINT_QUESTION_PROMPT,\n",
    "    ZERO_SHOT_MARKET_PROMPT,\n",
    "    ZERO_SHOT_NON_MARKET_JOINT_QUESTION_PROMPT,\n",
    "    ZERO_SHOT_NON_MARKET_PROMPT,\n",
    ")\n",
    "\n",
    "sys.path.append(os.path.join(current_path, \"../../..\"))  # noqa: E402\n",
    "from utils import gcp  # noqa: E402\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c71ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_joint_prompts = [\n",
    "    HUMAN_JOINT_PROMPT_1,\n",
    "    HUMAN_JOINT_PROMPT_2,\n",
    "    HUMAN_JOINT_PROMPT_3,\n",
    "    HUMAN_JOINT_PROMPT_4,\n",
    "]\n",
    "\n",
    "models = constants.ZERO_SHOT_AND_SCRATCHPAD_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d787e6",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_file = \"latest-llm.json\"\n",
    "\n",
    "with open(questions_file, \"r\") as file:\n",
    "    questions_data = json.load(file)\n",
    "\n",
    "questions = questions_data[\"questions\"]\n",
    "forecast_due_date = questions_data[\"forecast_due_date\"]\n",
    "base_file_path = \"individual_forecast_records/\" + forecast_due_date\n",
    "question_types = [\"market\", \"non_market\", \"combo_market\", \"combo_non_market\", \"final\"]\n",
    "\n",
    "single_market_questions = [\n",
    "    q\n",
    "    for q in questions\n",
    "    if q[\"combination_of\"] == \"N/A\" and q[\"source\"] not in question_curation.DATA_SOURCES\n",
    "]\n",
    "single_non_market_questions = [\n",
    "    q\n",
    "    for q in questions\n",
    "    if q[\"combination_of\"] == \"N/A\" and q[\"source\"] in question_curation.DATA_SOURCES\n",
    "]\n",
    "\n",
    "combo_market_questions = [\n",
    "    q\n",
    "    for q in questions\n",
    "    if q[\"combination_of\"] != \"N/A\" and q[\"source\"] not in question_curation.DATA_SOURCES\n",
    "]\n",
    "combo_non_market_questions = [\n",
    "    q\n",
    "    for q in questions\n",
    "    if q[\"combination_of\"] != \"N/A\" and q[\"source\"] in question_curation.DATA_SOURCES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d850050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll(combo_questions):\n",
    "    \"\"\"Unroll combo questions by directions.\"\"\"\n",
    "    combo_questions_unrolled = []\n",
    "    for q in combo_questions:\n",
    "        for i in range(4):\n",
    "            new_q = q.copy()\n",
    "            new_q[\"combo_index\"] = i\n",
    "\n",
    "            combo_questions_unrolled.append(new_q)\n",
    "    return combo_questions_unrolled\n",
    "\n",
    "\n",
    "combo_market_questions_unrolled = unroll(combo_market_questions)\n",
    "combo_non_market_questions_unrolled = unroll(combo_non_market_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adcf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(single_market_questions),\n",
    "    len(single_non_market_questions),\n",
    "    len(combo_market_questions_unrolled),\n",
    "    len(combo_non_market_questions_unrolled),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb486ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "single_market_questions = single_market_questions[:2]\n",
    "single_non_market_questions = single_non_market_questions[:2]\n",
    "combo_market_questions_unrolled = combo_market_questions_unrolled[:2]\n",
    "combo_non_market_questions_unrolled = combo_non_market_questions_unrolled[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6cbc4f",
   "metadata": {},
   "source": [
    "### Zero Shot Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(index, model_name, save_dict, questions_to_eval, rate_limit=False):\n",
    "    \"\"\"One worker for one zero shot question eval.\"\"\"\n",
    "    if save_dict[index] != \"\":\n",
    "        return\n",
    "\n",
    "    logger.info(f\"{model_name} - {index}\")\n",
    "\n",
    "    if rate_limit:\n",
    "        start_time = datetime.now()\n",
    "\n",
    "    if questions_to_eval[index][\"source\"] not in question_curation.DATA_SOURCES:\n",
    "        if questions_to_eval[index][\"combination_of\"] == \"N/A\":\n",
    "            prompt = ZERO_SHOT_MARKET_PROMPT.format(\n",
    "                question=questions_to_eval[index][\"question\"],\n",
    "                background=questions_to_eval[index][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria=questions_to_eval[index][\"resolution_criteria\"],\n",
    "                close_date=questions_to_eval[index][\"market_info_close_datetime\"],\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=100,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "            save_dict[index] = model_eval.extract_probability(response)\n",
    "        else:\n",
    "            prompt = ZERO_SHOT_MARKET_JOINT_QUESTION_PROMPT.format(\n",
    "                human_prompt=human_joint_prompts[questions_to_eval[index][\"combo_index\"]],\n",
    "                question_1=questions_to_eval[index][\"combination_of\"][0][\"question\"],\n",
    "                question_2=questions_to_eval[index][\"combination_of\"][1][\"question\"],\n",
    "                background_1=questions_to_eval[index][\"combination_of\"][0][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][0][\"market_info_resolution_criteria\"],\n",
    "                background_2=questions_to_eval[index][\"combination_of\"][1][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][1][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                resolution_criteria_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                close_date=max(\n",
    "                    questions_to_eval[index][\"combination_of\"][0][\"market_info_close_datetime\"],\n",
    "                    questions_to_eval[index][\"combination_of\"][1][\"market_info_close_datetime\"],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=100,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "\n",
    "            save_dict[index] = model_eval.extract_probability(response)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if questions_to_eval[index][\"combination_of\"] == \"N/A\":\n",
    "            prompt = ZERO_SHOT_NON_MARKET_PROMPT.format(\n",
    "                question=questions_to_eval[index][\"question\"],\n",
    "                background=questions_to_eval[index][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria=questions_to_eval[index][\"resolution_criteria\"],\n",
    "                freeze_datetime=questions_to_eval[index][\"freeze_datetime\"],\n",
    "                freeze_datetime_value=questions_to_eval[index][\"freeze_datetime_value\"],\n",
    "                freeze_datetime_value_explanation=questions_to_eval[index][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                list_of_resolution_dates=questions_to_eval[index][\"resolution_dates\"],\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=100,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "\n",
    "            save_dict[index] = model_eval.reformat_answers(\n",
    "                response=response, prompt=prompt, question=questions_to_eval[index]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            prompt = ZERO_SHOT_NON_MARKET_JOINT_QUESTION_PROMPT.format(\n",
    "                human_prompt=human_joint_prompts[questions_to_eval[index][\"combo_index\"]],\n",
    "                question_1=questions_to_eval[index][\"combination_of\"][0][\"question\"],\n",
    "                question_2=questions_to_eval[index][\"combination_of\"][1][\"question\"],\n",
    "                background_1=questions_to_eval[index][\"combination_of\"][0][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][0][\"market_info_resolution_criteria\"],\n",
    "                background_2=questions_to_eval[index][\"combination_of\"][1][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][1][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                resolution_criteria_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                freeze_datetime_1=questions_to_eval[index][\"combination_of\"][0][\"freeze_datetime\"],\n",
    "                freeze_datetime_2=questions_to_eval[index][\"combination_of\"][1][\"freeze_datetime\"],\n",
    "                freeze_datetime_value_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"freeze_datetime_value\"\n",
    "                ],\n",
    "                freeze_datetime_value_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"freeze_datetime_value\"\n",
    "                ],\n",
    "                freeze_datetime_value_explanation_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                freeze_datetime_value_explanation_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                list_of_resolution_dates=questions_to_eval[index][\"resolution_dates\"],\n",
    "            )\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=500,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "\n",
    "            save_dict[index] = model_eval.reformat_answers(\n",
    "                response=response, prompt=prompt, question=questions_to_eval[index]\n",
    "            )\n",
    "\n",
    "    logger.info(f\"Model: {model_name} | Answer: {save_dict[index]}\")\n",
    "\n",
    "    if rate_limit:\n",
    "        end_time = datetime.now()\n",
    "        elapsed_time = (end_time - start_time).total_seconds()\n",
    "        if elapsed_time < 1:\n",
    "            time.sleep(\n",
    "                1 - elapsed_time\n",
    "            )  # Ensure at least 1 second per request to stay within rate limits\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def executor(max_workers, model_name, save_dict, questions_to_eval):\n",
    "    \"\"\"Zero shot executor.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        worker_with_args = partial(\n",
    "            worker, model_name=model_name, save_dict=save_dict, questions_to_eval=questions_to_eval\n",
    "        )\n",
    "        return list(executor.map(worker_with_args, range(len(questions_to_eval))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4cc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "model_result_loaded = {}\n",
    "models_to_test = list(models.keys())\n",
    "prompt_type = \"zero_shot\"\n",
    "\n",
    "for question in [\n",
    "    single_non_market_questions,\n",
    "    single_market_questions,\n",
    "    combo_market_questions_unrolled,\n",
    "    combo_non_market_questions_unrolled,\n",
    "]:\n",
    "    questions_to_eval = question\n",
    "    if question[0][\"source\"] not in question_curation.DATA_SOURCES:\n",
    "        if question[0][\"combination_of\"] == \"N/A\":\n",
    "            test_type = f\"{prompt_type}/market\"\n",
    "        else:\n",
    "            test_type = f\"{prompt_type}/combo_market\"\n",
    "    else:\n",
    "        if question[0][\"combination_of\"] == \"N/A\":\n",
    "            test_type = f\"{prompt_type}/non_market\"\n",
    "        else:\n",
    "            test_type = f\"{prompt_type}/combo_non_market\"\n",
    "\n",
    "    for model in models_to_test:\n",
    "        if model not in model_result_loaded.keys():\n",
    "            model_result_loaded[model] = {}\n",
    "        model_result_loaded[model] = False\n",
    "\n",
    "    for model in models_to_test:\n",
    "        gcp_file_path = f\"{base_file_path}/{test_type}/{model}.jsonl\"\n",
    "        if model not in results.keys():\n",
    "            results[model] = {}\n",
    "        try:\n",
    "            results[model] = data_utils.download_and_read_saved_forecasts(\n",
    "                gcp_file_path, base_file_path\n",
    "            )\n",
    "            model_result_loaded[model] = True  # Set flag to True if loaded successfully\n",
    "            print(f\"Downloaded {gcp_file_path}.\")\n",
    "        except Exception:  # Catching general exceptions\n",
    "            results[model] = {i: \"\" for i in range(len(questions_to_eval))}\n",
    "\n",
    "    for model in models_to_test:\n",
    "        local_filename = f\"{test_type}/{model}.jsonl\"\n",
    "        if not model_result_loaded[model]:\n",
    "            executor_count = 50\n",
    "            if models[model][\"source\"] == \"ANTHROPIC\":\n",
    "                executor_count = 30\n",
    "            elif models[model][\"source\"] == \"GOOGLE\":\n",
    "                executor_count = 10\n",
    "\n",
    "            executor(executor_count, model, results[model], questions_to_eval)\n",
    "\n",
    "            current_model_forecasts = []\n",
    "            for index in range(len(questions_to_eval)):\n",
    "                if questions_to_eval[index][\"source\"] in question_curation.DATA_SOURCES:\n",
    "                    for forecast, resolution_date in zip(\n",
    "                        results[model][index], questions_to_eval[index][\"resolution_dates\"]\n",
    "                    ):\n",
    "                        current_forecast = {\n",
    "                            \"id\": questions_to_eval[index][\"id\"],\n",
    "                            \"source\": questions_to_eval[index][\"source\"],\n",
    "                            \"forecast\": forecast,\n",
    "                            \"resolution_date\": resolution_date,\n",
    "                            \"reasoning\": None,\n",
    "                        }\n",
    "                        if questions_to_eval[index][\"combination_of\"] != \"N/A\":\n",
    "                            combo_index = questions_to_eval[index][\"combo_index\"]\n",
    "                            if combo_index == 0:\n",
    "                                current_forecast[\"direction\"] = [1, 1]\n",
    "                            elif combo_index == 1:\n",
    "                                current_forecast[\"direction\"] = [1, -1]\n",
    "                            elif combo_index == 2:\n",
    "                                current_forecast[\"direction\"] = [-1, 1]\n",
    "                            else:\n",
    "                                current_forecast[\"direction\"] = [-1, -1]\n",
    "\n",
    "                        current_model_forecasts.append(current_forecast)\n",
    "                else:\n",
    "                    current_forecast = {\n",
    "                        \"id\": questions_to_eval[index][\"id\"],\n",
    "                        \"source\": questions_to_eval[index][\"source\"],\n",
    "                        \"forecast\": results[model][index],\n",
    "                        \"reasoning\": None,\n",
    "                    }\n",
    "                    current_model_forecasts.append(current_forecast)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_filename), exist_ok=True)\n",
    "            with open(local_filename, \"w\") as file:\n",
    "                for entry in current_model_forecasts:\n",
    "                    json_line = json.dumps(entry)\n",
    "                    file.write(json_line + \"\\n\")\n",
    "\n",
    "            gcp.storage.upload(\n",
    "                bucket_name=env.FORECAST_SETS_BUCKET,\n",
    "                local_filename=local_filename,\n",
    "                filename=f\"{base_file_path}/\" + local_filename,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46760c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.generage_final_forecast_files(\n",
    "    deadline=forecast_due_date, prompt_type=prompt_type, models=models\n",
    ")\n",
    "\n",
    "data_utils.delete_and_upload_to_the_cloud(base_file_path, prompt_type, question_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bd6dc",
   "metadata": {},
   "source": [
    "### Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23471fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(index, model_name, save_dict, questions_to_eval):  # noqa: F811\n",
    "    \"\"\"Scratchpad worker.\"\"\"\n",
    "    if save_dict[index] != \"\":\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Starting {model_name} - {index}\")\n",
    "\n",
    "    if questions_to_eval[index][\"source\"] not in question_curation.DATA_SOURCES:\n",
    "        if questions_to_eval[index][\"combination_of\"] == \"N/A\":\n",
    "            prompt = SCRATCH_PAD_MARKET_PROMPT.format(\n",
    "                question=questions_to_eval[index][\"question\"],\n",
    "                background=questions_to_eval[index][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria=questions_to_eval[index][\"resolution_criteria\"],\n",
    "                close_date=questions_to_eval[index][\"market_info_close_datetime\"],\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=1300,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "\n",
    "            save_dict[index] = (\n",
    "                model_eval.reformat_answers(response=response, single=True),\n",
    "                response,\n",
    "            )\n",
    "        else:\n",
    "            prompt = SCRATCH_PAD_MARKET_JOINT_QUESTION_PROMPT.format(\n",
    "                human_prompt=human_joint_prompts[questions_to_eval[index][\"combo_index\"]],\n",
    "                question_1=questions_to_eval[index][\"combination_of\"][0][\"question\"],\n",
    "                question_2=questions_to_eval[index][\"combination_of\"][1][\"question\"],\n",
    "                background_1=questions_to_eval[index][\"combination_of\"][0][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][0][\"market_info_resolution_criteria\"],\n",
    "                background_2=questions_to_eval[index][\"combination_of\"][1][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][1][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                resolution_criteria_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                close_date=max(\n",
    "                    questions_to_eval[index][\"combination_of\"][0][\"market_info_close_datetime\"],\n",
    "                    questions_to_eval[index][\"combination_of\"][1][\"market_info_close_datetime\"],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=1300,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "\n",
    "            save_dict[index] = (\n",
    "                model_eval.reformat_answers(response=response, single=True),\n",
    "                response,\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        if questions_to_eval[index][\"combination_of\"] == \"N/A\":\n",
    "            prompt = SCRATCH_PAD_NON_MARKET_PROMPT.format(\n",
    "                question=questions_to_eval[index][\"question\"],\n",
    "                background=questions_to_eval[index][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria=questions_to_eval[index][\"resolution_criteria\"],\n",
    "                freeze_datetime=questions_to_eval[index][\"freeze_datetime\"],\n",
    "                freeze_datetime_value=questions_to_eval[index][\"freeze_datetime_value\"],\n",
    "                freeze_datetime_value_explanation=questions_to_eval[index][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                list_of_resolution_dates=questions_to_eval[index][\"resolution_dates\"],\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=2000,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "\n",
    "            save_dict[index] = (\n",
    "                model_eval.reformat_answers(\n",
    "                    response=response, prompt=prompt, question=questions_to_eval[index]\n",
    "                ),\n",
    "                response,\n",
    "            )\n",
    "        else:\n",
    "            prompt = SCRATCH_PAD_NON_MARKET_JOINT_QUESTION_PROMPT.format(\n",
    "                human_prompt=human_joint_prompts[questions_to_eval[index][\"combo_index\"]],\n",
    "                question_1=questions_to_eval[index][\"combination_of\"][0][\"question\"],\n",
    "                question_2=questions_to_eval[index][\"combination_of\"][1][\"question\"],\n",
    "                background_1=questions_to_eval[index][\"combination_of\"][0][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][0][\"market_info_resolution_criteria\"],\n",
    "                background_2=questions_to_eval[index][\"combination_of\"][1][\"background\"]\n",
    "                + \"\\n\"\n",
    "                + questions_to_eval[index][\"combination_of\"][1][\"market_info_resolution_criteria\"],\n",
    "                resolution_criteria_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                resolution_criteria_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"resolution_criteria\"\n",
    "                ],\n",
    "                freeze_datetime_1=questions_to_eval[index][\"combination_of\"][0][\"freeze_datetime\"],\n",
    "                freeze_datetime_2=questions_to_eval[index][\"combination_of\"][1][\"freeze_datetime\"],\n",
    "                freeze_datetime_value_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"freeze_datetime_value\"\n",
    "                ],\n",
    "                freeze_datetime_value_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"freeze_datetime_value\"\n",
    "                ],\n",
    "                freeze_datetime_value_explanation_1=questions_to_eval[index][\"combination_of\"][0][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                freeze_datetime_value_explanation_2=questions_to_eval[index][\"combination_of\"][1][\n",
    "                    \"freeze_datetime_value_explanation\"\n",
    "                ],\n",
    "                list_of_resolution_dates=questions_to_eval[index][\"resolution_dates\"],\n",
    "            )\n",
    "            try:\n",
    "                response = model_eval.get_response_from_model(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=2000,\n",
    "                    model_name=models[model_name][\"full_name\"],\n",
    "                    temperature=0,\n",
    "                    wait_time=30,\n",
    "                )\n",
    "            except Exception as e:  # Catching general exceptions and printing the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                response = None\n",
    "\n",
    "            save_dict[index] = (\n",
    "                model_eval.reformat_answers(\n",
    "                    response=response, prompt=prompt, question=questions_to_eval[index]\n",
    "                ),\n",
    "                response,\n",
    "            )\n",
    "\n",
    "    logger.info(f\"Model: {model_name} | Answer: {save_dict[index][0]}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def executor(max_workers, model_name, save_dict, questions_to_eval):\n",
    "    \"\"\"Scratchpad executor.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        worker_with_args = partial(\n",
    "            worker, model_name=model_name, save_dict=save_dict, questions_to_eval=questions_to_eval\n",
    "        )\n",
    "        return list(executor.map(worker_with_args, range(len(questions_to_eval))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "model_result_loaded = {}\n",
    "models_to_test = list(models.keys())[:]\n",
    "prompt_type = \"scratchpad\"\n",
    "\n",
    "for question in [\n",
    "    single_market_questions,\n",
    "    single_non_market_questions,\n",
    "    combo_market_questions_unrolled,\n",
    "    combo_non_market_questions_unrolled,\n",
    "]:\n",
    "    questions_to_eval = question\n",
    "    if question[0][\"source\"] not in question_curation.DATA_SOURCES:\n",
    "        if question[0][\"combination_of\"] == \"N/A\":\n",
    "            test_type = f\"{prompt_type}/market\"\n",
    "        else:\n",
    "            test_type = f\"{prompt_type}/combo_market\"\n",
    "    else:\n",
    "        if question[0][\"combination_of\"] == \"N/A\":\n",
    "            test_type = f\"{prompt_type}/non_market\"\n",
    "        else:\n",
    "            test_type = f\"{prompt_type}/combo_non_market\"\n",
    "\n",
    "    for model in models_to_test:\n",
    "        if model not in model_result_loaded.keys():\n",
    "            model_result_loaded[model] = {}\n",
    "        model_result_loaded[model] = False\n",
    "\n",
    "    for model in models_to_test:\n",
    "        gcp_file_path = f\"{base_file_path}/{test_type}/{model}.jsonl\"\n",
    "        if model not in results.keys():\n",
    "            results[model] = {}\n",
    "        try:\n",
    "            results[model] = data_utils.download_and_read_saved_forecasts(\n",
    "                gcp_file_path, base_file_path\n",
    "            )\n",
    "            model_result_loaded[model] = True  # Set flag to True if loaded successfully\n",
    "            print(f\"Downloaded {gcp_file_path}.\")\n",
    "        except Exception:  # Catching general exceptions\n",
    "            results[model] = {i: \"\" for i in range(len(questions_to_eval))}\n",
    "\n",
    "    for model in models_to_test:\n",
    "        local_filename = f\"{test_type}/{model}.jsonl\"\n",
    "        if not model_result_loaded[model]:\n",
    "            executor_count = 50\n",
    "            if models[model][\"source\"] == \"ANTHROPIC\":\n",
    "                executor_count = 30\n",
    "            elif models[model][\"source\"] == \"GOOGLE\":\n",
    "                executor_count = 10\n",
    "\n",
    "            executor(executor_count, model, results[model], questions_to_eval)\n",
    "\n",
    "            current_model_forecasts = []\n",
    "            for index in range(len(questions_to_eval)):\n",
    "                if questions_to_eval[index][\"source\"] in question_curation.DATA_SOURCES:\n",
    "                    for forecast, resolution_date in zip(\n",
    "                        results[model][index][0], questions_to_eval[index][\"resolution_dates\"]\n",
    "                    ):\n",
    "                        current_forecast = {\n",
    "                            \"id\": questions_to_eval[index][\"id\"],\n",
    "                            \"source\": questions_to_eval[index][\"source\"],\n",
    "                            \"forecast\": forecast,\n",
    "                            \"resolution_date\": resolution_date,\n",
    "                            \"reasoning\": results[model][index][1],\n",
    "                        }\n",
    "\n",
    "                        if questions_to_eval[index][\"combination_of\"] != \"N/A\":\n",
    "                            combo_index = questions_to_eval[index][\"combo_index\"]\n",
    "                            if combo_index == 0:\n",
    "                                current_forecast[\"direction\"] = [1, 1]\n",
    "                            elif combo_index == 1:\n",
    "                                current_forecast[\"direction\"] = [1, -1]\n",
    "                            elif combo_index == 2:\n",
    "                                current_forecast[\"direction\"] = [-1, 1]\n",
    "                            else:\n",
    "                                current_forecast[\"direction\"] = [-1, -1]\n",
    "\n",
    "                        current_model_forecasts.append(current_forecast)\n",
    "\n",
    "                else:\n",
    "                    current_forecast = {\n",
    "                        \"id\": questions_to_eval[index][\"id\"],\n",
    "                        \"source\": questions_to_eval[index][\"source\"],\n",
    "                        \"forecast\": results[model][index][0],\n",
    "                        \"reasoning\": results[model][index][1],\n",
    "                    }\n",
    "                    current_model_forecasts.append(current_forecast)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_filename), exist_ok=True)\n",
    "            with open(local_filename, \"w\") as file:\n",
    "                for entry in current_model_forecasts:\n",
    "                    json_line = json.dumps(entry)\n",
    "                    file.write(json_line + \"\\n\")\n",
    "\n",
    "            gcp.storage.upload(\n",
    "                bucket_name=env.FORECAST_SETS_BUCKET,\n",
    "                local_filename=local_filename,\n",
    "                filename=f\"{base_file_path}/\" + local_filename,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.generage_final_forecast_files(\n",
    "    deadline=forecast_due_date, prompt_type=prompt_type, models=models\n",
    ")\n",
    "\n",
    "data_utils.delete_and_upload_to_the_cloud(base_file_path, prompt_type, question_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
