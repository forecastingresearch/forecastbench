\begin{table}[ht!]
\centering
\resizebox{\textwidth}{!} {%
\Large
\begin{tabular}{@{}l l l l >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm}@{}}
\toprule
& & & & \multicolumn{3}{c}{Brier Score $\downarrow$} \\ \cmidrule(lr){5-7}
Model & Organization & \begin{tabular}[c]{@{}l@{}}Information\\ provided\end{tabular} & Prompt & \begin{tabular}[c]{@{}l@{}}Dataset\\($N$=422)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Market\\($N$=76)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Overall\\($N$=498)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Confidence\\ Interval\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pairwise\\$p$-value\\comparing\\to No. 1\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pct. more \\accurate\\ than No. 1\end{tabular} \\
\midrule
Superforecaster median forecast & ForecastBench & -- & -- & 0.118 & 0.074 & 0.096 & [0.076, 0.116] & -- & 0\% \\
Public median forecast & ForecastBench & -- & -- & 0.153 & 0.089 & 0.121 & [0.101, 0.141] & <0.001 & 22\% \\
Claude-3-5-Sonnet-20240620 & Anthropic & Freeze values & Scratchpad & 0.138 & 0.107 & 0.122 & [0.099, 0.146] & <0.001 & 31\% \\
Claude-3-5-Sonnet-20240620 & Anthropic & News with freeze values & Scratchpad & 0.142 & 0.112 & 0.127 & [0.104, 0.150] & <0.001 & 29\% \\
GPT-4-Turbo-2024-04-09 & OpenAI & Freeze values & Zero shot & 0.162 & 0.095 & 0.128 & [0.105, 0.151] & <0.001 & 32\% \\
Claude-3-5-Sonnet-20240620 & Anthropic & Freeze values & Zero shot & 0.145 & 0.117 & 0.131 & [0.103, 0.159] & <0.001 & 31\% \\
GPT-4 & OpenAI & Freeze values & Zero shot & 0.167 & 0.096 & 0.132 & [0.109, 0.155] & <0.001 & 31\% \\
GPT-4o & OpenAI & News with freeze values & Scratchpad & 0.162 & 0.105 & 0.133 & [0.113, 0.154] & <0.001 & 25\% \\
Claude-3-5-Sonnet-20240620 & Anthropic & -- & Scratchpad & 0.138 & 0.133 & 0.136 & [0.113, 0.158] & <0.001 & 28\% \\
GPT-4o & OpenAI & Freeze values & Scratchpad & 0.161 & 0.113 & 0.137 & [0.115, 0.158] & <0.001 & 27\% \\
\bottomrule
\end{tabular}
}
\begin{minipage}{\textwidth}
{\tiny
    \textit{Notes:} \\
    \vspace{-2mm}
    1. Shows performance on the 200 standard questions provided in the human question set at the 7-, 30-, 90-, and 180-day forecast horizons.\\
    \vspace{-2mm} % reduces space between this line and the next one
    2. For resolved market questions, forecasts are compared against ground truth while for unresolved market questions, they are compared to community aggregates.\\
    \vspace{-2mm} % reduces space between this line and the next one
    3. The overall score is calculated as the average of the mean dataset Brier score and the mean market Brier score.\\
    \vspace{-2mm} % reduces space between this line and the next one
    4. Pairwise $p$-value comparing to No. 1 (bootstrapped): The $p$-value calculated by bootstrapping the differences in overall score between each model and the best\\ \vspace{-2mm} forecaster under the null hypothesis that there's no difference.\\
    \vspace{-2mm}
    5. Pct. more accurate than No. 1: The percent of questions where this forecaster had a better overall score than the best forecaster.\\
}
\end{minipage}
\end{table}
